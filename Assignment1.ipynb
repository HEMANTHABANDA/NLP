{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJYj5QHF0ylepsMev/DsnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HEMANTHABANDA/NLP/blob/main/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_jzs4anuzAx",
        "outputId": "e7cc9fe0-9bbc-43b5-a9fd-42919566ead0"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGuRKhvpwvLF",
        "outputId": "0ee5b7b0-9ec1-4379-b413-f8bfbc21137f"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text_data= '''Are  you  fascinated  by  the  amount  of  text  data  available  on  the  internet?  Are  you \n",
        "looking  for  ways  to  work  with  this  text  data  but  aren’t  sure  where  to  begin? \n",
        "Machines, after all, recognize numbers, not the letters of our language. And that can \n",
        "be a tricky landscape to navigate in machine learning.'''\n",
        "print(sent_tokenize(text_data))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Are  you  fascinated  by  the  amount  of  text  data  available  on  the  internet?', 'Are  you \\nlooking  for  ways  to  work  with  this  text  data  but  aren’t  sure  where  to  begin?', 'Machines, after all, recognize numbers, not the letters of our language.', 'And that can \\nbe a tricky landscape to navigate in machine learning.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6QJT9auw2Nu",
        "outputId": "2facf7eb-fbcb-417b-b8ee-b04e1d39e3f8"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text =  '''Are  you  fascinated  by  the  amount  of  text  data  available  on  the  internet?  Are  you \n",
        "looking  for  ways  to  work  with  this  text  data  but  aren’t  sure  where  to  begin? \n",
        "Machines, after all, recognize numbers, not the letters of our language. And that can \n",
        "be a tricky landscape to navigate in machine learning.'''\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Are', 'you', 'fascinated', 'by', 'the', 'amount', 'of', 'text', 'data', 'available', 'on', 'the', 'internet', '?', 'Are', 'you', 'looking', 'for', 'ways', 'to', 'work', 'with', 'this', 'text', 'data', 'but', 'aren', '’', 't', 'sure', 'where', 'to', 'begin', '?', 'Machines', ',', 'after', 'all', ',', 'recognize', 'numbers', ',', 'not', 'the', 'letters', 'of', 'our', 'language', '.', 'And', 'that', 'can', 'be', 'a', 'tricky', 'landscape', 'to', 'navigate', 'in', 'machine', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pusIFJzexfgd",
        "outputId": "ab8f81a2-81c4-4e64-dfd5-26c271ac6211"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter_stemmer  = PorterStemmer()\n",
        "words = ['cats','trouble','troubling','troubled','having','Corriendo','at','was']\n",
        "print(\"stemming for :\")\n",
        "for w in words:\n",
        "    print(\"   {} is {}\".format(w,porter_stemmer.stem(w)))  "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stemming for :\n",
            "   cats is cat\n",
            "   trouble is troubl\n",
            "   troubling is troubl\n",
            "   troubled is troubl\n",
            "   having is have\n",
            "   Corriendo is corriendo\n",
            "   at is at\n",
            "   was is wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvJ9Msi9xwFj",
        "outputId": "fe5b32e1-33ec-4bbe-9639-6243514c6208"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw4btivkxmV8",
        "outputId": "4a83aee7-53cd-4d6a-f834-6412cbaa1c59"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "words = ['cats','trouble','troubling','troubled','having','Corriendo','at','was']\n",
        "print(\"lemma for :\")\n",
        "for w in words:\n",
        "    print(\"   {} is {}\".format(w,w, wordnet_lemmatizer.lemmatize(w)))  \n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemma for :\n",
            "   cats is cats\n",
            "   trouble is trouble\n",
            "   troubling is troubling\n",
            "   troubled is troubled\n",
            "   having is having\n",
            "   Corriendo is Corriendo\n",
            "   at is at\n",
            "   was is was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuE4ACfLx_dA"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MezvayPLyDax"
      },
      "source": [
        "para = '''he NLTK library  is  one  of  the  oldest  and  most  commonly  used  Python  libraries  for \n",
        "Natural Language Processing. NLTK supports stop word removal, and you can find the list \n",
        "of stop words in the  corpus  module. To remove stop words from a sentence, you can divide \n",
        "your text into words and then remove the word if it exits in the list of stop words provided \n",
        "by NLTK.'''"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhybUNMtySav",
        "outputId": "2c3ce6ac-dd49-4e77-ef82-6aa485172a9b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-E4n4TIyIHJ"
      },
      "source": [
        "stopwords = set(stopwords.words('english'))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB0Y45TOyawk"
      },
      "source": [
        "words = word_tokenize(para)\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqrZtXMKyd1a"
      },
      "source": [
        "res = [w for w in words if not w.lower() in stopwords]\n",
        " "
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bOlik5iyiCr",
        "outputId": "debad683-6581-4da2-f317-578f20239a11"
      },
      "source": [
        "res = []\n",
        " \n",
        "for w in words:\n",
        "    if w not in stopwords:\n",
        "        res.append(w)\n",
        " \n",
        "#print(words)\n",
        "print(res)\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'library', 'one', 'oldest', 'commonly', 'used', 'Python', 'libraries', 'Natural', 'Language', 'Processing', '.', 'NLTK', 'supports', 'stop', 'word', 'removal', ',', 'find', 'list', 'stop', 'words', 'corpus', 'module', '.', 'To', 'remove', 'stop', 'words', 'sentence', ',', 'divide', 'text', 'words', 'remove', 'word', 'exits', 'list', 'stop', 'words', 'provided', 'NLTK', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntOKVXaqyl9g"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import webtext\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE9CUeUNyqhh"
      },
      "source": [
        "para = '''he NLTK library  is  one  of  the  oldest  and  most  commonly  used  Python  libraries  for \n",
        "Natural Language Processing. NLTK supports stop word removal, and you can find the list \n",
        "of stop words in the  corpus  module. To remove stop words from a sentence, you can divide \n",
        "your text into words and then remove the word if it exits in the list of stop words provided \n",
        "by NLTK.'''"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7ZwWkZ-yusp"
      },
      "source": [
        "words = word_tokenize(para)\n",
        "data_analysis = nltk.FreqDist(words)\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4dzjgttyxRE"
      },
      "source": [
        "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
        " "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjYRm2CQy0Oz",
        "outputId": "84f82c83-0d68-41e0-af73-8b319f491568"
      },
      "source": [
        "for key in sorted(filter_words):\n",
        "    print(\"%s: %s\" % (key, filter_words[key]))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language: 1\n",
            "NLTK: 3\n",
            "Natural: 1\n",
            "Processing: 1\n",
            "Python: 1\n",
            "commonly: 1\n",
            "corpus: 1\n",
            "divide: 1\n",
            "exits: 1\n",
            "find: 1\n",
            "from: 1\n",
            "into: 1\n",
            "libraries: 1\n",
            "library: 1\n",
            "list: 2\n",
            "module: 1\n",
            "most: 1\n",
            "oldest: 1\n",
            "provided: 1\n",
            "removal: 1\n",
            "remove: 2\n",
            "sentence: 1\n",
            "stop: 4\n",
            "supports: 1\n",
            "text: 1\n",
            "then: 1\n",
            "used: 1\n",
            "word: 2\n",
            "words: 4\n",
            "your: 1\n"
          ]
        }
      ]
    }
  ]
}